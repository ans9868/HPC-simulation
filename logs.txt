[root@f1beba1b1841 eeg-full-pipeline]# python start-pipelines.py
ğŸ“ Using most recent config: config_macmini1.0.0_09-08-2025_1751.yaml
ğŸš€ Starting pipeline with deployment method: Singularity without Slurm
ğŸ¯ Pipeline mode: full
ğŸ’¡ Checking for required container images...
âœ… Found pyspark container: eeg-pyspark-pipeline.sif (1521.4 MB)
âœ… Found ray container: eeg-ray-tuner.sif (195.6 MB)

ğŸ”’ Running PySpark Singularity container...
ğŸ“ Created/verified directory: config
ğŸ“ Created/verified directory: config/spark
ğŸ“ Created/verified directory: logs
ğŸ“ Created/verified directory: logs/spark-events
ğŸ“ Created/verified directory: /eeg-full-pipeline/data
ğŸ”— Adding mount: /Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set -> /Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set
ğŸ”— Mounting 4 directories:
ğŸ”— Running command: ['singularity', 'run', '--bind', '/eeg-full-pipeline/config/config_macmini1.0.0_09-08-2025_1751.yaml:/app/config.yaml', '--bind', './logs/spark-events:/opt/bitnami/spark/logs/', '--bind', './data:/app/data', '--bind', '/Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set:/Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set', './containers/eeg-pyspark-pipeline.sif', 'spark-submit', '--conf', 'spark.jars.ivy=/tmp/.ivy2', '/app/main.py', '--config', '/app/config.yaml']
ğŸš€ Starting pyspark container... (this may take a while)
ğŸš€ [ENTRYPOINT] creating the .ivy2 folder and running the start command
ğŸš€ [ENTRYPOINT] Set HADOOP_USER_NAME=spark
ğŸš€ [ENTRYPOINT] Set the path and timestamp for the spark-driver log file
ğŸš€ [ENTRYPOINT] spark environment and logging configured
ğŸš€ [ENTRYPOINT] Running: spark-submit --conf spark.jars.ivy=/tmp/.ivy2 /app/main.py --config /app/config.yaml
ğŸš€ ğŸ”§ Loading configuration from: /app/config.yaml
ğŸš€ ğŸ“‹ Configuration loaded successfully!
ğŸš€ ğŸ” Checking mounted data accessibility...
ğŸš€ ğŸ“ Found 1 file paths to check
ğŸš€ âœ“ /Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set -> /Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set
ğŸš€ ğŸ“Š Data accessibility summary:
ğŸš€ Total paths: 1
ğŸš€ Accessible: 1
ğŸš€ Inaccessible: 0
ğŸš€ âœ… All data paths are accessible!
ğŸš€ ğŸš€ Building PySpark session...
ğŸš€ âœ… Found 'master': 7
ğŸš€ âœ… Found 'driver_memory': 7
ğŸš€ âœ… Found 'executor_memory': 7
ğŸš€ âœ… Found 'executor_cores': 3
ğŸš€ âœ… Found 'shuffle_partitions': 9
ğŸš€ 25/08/10 05:35:13 INFO SparkContext: Running Spark version 3.5.6
ğŸš€ 25/08/10 05:35:13 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
ğŸš€ 25/08/10 05:35:13 INFO SparkContext: Java version 17.0.15
ğŸš€ 25/08/10 05:35:13 INFO ResourceUtils: ==============================================================
ğŸš€ 25/08/10 05:35:13 INFO ResourceUtils: No custom resources configured for spark.driver.
ğŸš€ 25/08/10 05:35:13 INFO ResourceUtils: ==============================================================
ğŸš€ 25/08/10 05:35:13 INFO SparkContext: Submitted application: macMini2.0.0
ğŸš€ 25/08/10 05:35:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 7168, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
ğŸš€ 25/08/10 05:35:13 INFO ResourceProfile: Limiting resource is cpus at 3 tasks per executor
ğŸš€ 25/08/10 05:35:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
ğŸš€ 25/08/10 05:35:13 INFO SecurityManager: Changing view acls to: ?,spark
ğŸš€ 25/08/10 05:35:13 INFO SecurityManager: Changing modify acls to: ?,spark
ğŸš€ 25/08/10 05:35:13 INFO SecurityManager: Changing view acls groups to:
ğŸš€ 25/08/10 05:35:13 INFO SecurityManager: Changing modify acls groups to:
ğŸš€ 25/08/10 05:35:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ?, spark; groups with view permissions: EMPTY; users with modify permissions: ?, spark; groups with modify permissions: EMPTY
ğŸš€ 25/08/10 05:35:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
ğŸš€ 25/08/10 05:35:13 INFO Utils: Successfully started service 'sparkDriver' on port 43935.
ğŸš€ 25/08/10 05:35:13 INFO SparkEnv: Registering MapOutputTracker
ğŸš€ 25/08/10 05:35:13 INFO SparkEnv: Registering BlockManagerMaster
ğŸš€ 25/08/10 05:35:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
ğŸš€ 25/08/10 05:35:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
ğŸš€ 25/08/10 05:35:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
ğŸš€ 25/08/10 05:35:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b2a25d66-470b-4826-a5c9-95b93a0abe59
ğŸš€ 25/08/10 05:35:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
ğŸš€ 25/08/10 05:35:13 INFO SparkEnv: Registering OutputCommitCoordinator
ğŸš€ 25/08/10 05:35:13 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
ğŸš€ 25/08/10 05:35:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
ğŸš€ 25/08/10 05:35:13 INFO Executor: Starting executor ID driver on host f1beba1b1841
ğŸš€ 25/08/10 05:35:13 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
ğŸš€ 25/08/10 05:35:13 INFO Executor: Java version 17.0.15
ğŸš€ 25/08/10 05:35:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
ğŸš€ 25/08/10 05:35:13 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3f46bd39 for default.
ğŸš€ 25/08/10 05:35:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38967.
ğŸš€ 25/08/10 05:35:13 INFO NettyBlockTransferService: Server created on f1beba1b1841:38967
ğŸš€ 25/08/10 05:35:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
ğŸš€ 25/08/10 05:35:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f1beba1b1841, 38967, None)
ğŸš€ 25/08/10 05:35:13 INFO BlockManagerMasterEndpoint: Registering block manager f1beba1b1841:38967 with 434.4 MiB RAM, BlockManagerId(driver, f1beba1b1841, 38967, None)
ğŸš€ 25/08/10 05:35:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f1beba1b1841, 38967, None)
ğŸš€ 25/08/10 05:35:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f1beba1b1841, 38967, None)
ğŸš€ ğŸš€ Local mode detected - skipping file distribution
ğŸš€ Package files available through container filesystem
ğŸš€ âœ… PySpark session created successfully!
ğŸš€ Spark Version: 3.5.6
ğŸš€ Available Cores: 7
ğŸš€ Master: local[7]
ğŸš€ ğŸ”„ Passing config to process_subjects via function parameters...
ğŸš€ ğŸ“ Creating output directory structure...
ğŸš€ Base directory: ./data
ğŸš€ Project name: macMini2.0.0
ğŸš€ Full path: data/macMini2.0.0
ğŸš€ âœ… Output directory created: data/macMini2.0.0
ğŸš€ ğŸ“ Creating stage directories...
ğŸš€ âœ… Created: data/macMini2.0.0/raw
ğŸš€ âœ… Created: data/macMini2.0.0/processed_subjects
ğŸš€ âœ… Created: data/macMini2.0.0/transformed
ğŸš€ ğŸ“‹ Config copied to output directory:
ğŸš€ Source: /app/config.yaml
ğŸš€ Destination: data/macMini2.0.0/config_20250810_053513.yaml
ğŸš€ ğŸ”„ Reuse not enabled - proceeding with processing...
ğŸš€ âœ… process_subjects: Using passed ConfigManager
ğŸš€ ğŸ“‹ ConfigManager project: macMini2.0.0
ğŸš€ ğŸ”§ Validating configuration...
ğŸš€ ğŸ”„ Processing subjects using DataFrames...
ğŸš€ 25/08/10 05:35:14 WARN FileSystem: Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.viewfs.ViewFileSystem could not be instantiated
ğŸš€ 25/08/10 05:35:14 WARN FileSystem: org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
ğŸš€ at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(Unknown Source)
ğŸš€ at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invoke(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/java.security.AccessController.doPrivileged(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invokePriv(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.login(Unknown Source)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
ğŸš€ at org.apache.hadoop.fs.viewfs.ViewFileSystem.<init>(ViewFileSystem.java:269)
ğŸš€ at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
ğŸš€ at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(Unknown Source)
ğŸš€ at java.base/java.util.ServiceLoader$ProviderImpl.get(Unknown Source)
ğŸš€ at java.base/java.util.ServiceLoader$3.next(Unknown Source)
ğŸš€ at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3379)
ğŸš€ at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3424)
ğŸš€ at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>(FsUrlStreamHandlerFactory.java:77)
ğŸš€ at org.apache.spark.sql.internal.SharedState$.liftedTree2$1(SharedState.scala:199)
ğŸš€ at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$setFsUrlStreamHandlerFactory(SharedState.scala:198)
ğŸš€ at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:54)
ğŸš€ at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
ğŸš€ at scala.Option.getOrElse(Option.scala:189)
ğŸš€ at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
ğŸš€ at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
ğŸš€ at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
ğŸš€ at scala.Option.getOrElse(Option.scala:189)
ğŸš€ at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
ğŸš€ at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
ğŸš€ at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
ğŸš€ at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
ğŸš€ at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
ğŸš€ at org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:572)
ğŸš€ at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:877)
ğŸš€ at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:862)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Method.invoke(Unknown Source)
ğŸš€ at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
ğŸš€ at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
ğŸš€ at py4j.Gateway.invoke(Gateway.java:282)
ğŸš€ at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
ğŸš€ at py4j.commands.CallCommand.execute(CallCommand.java:79)
ğŸš€ at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
ğŸš€ at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
ğŸš€ at java.base/java.lang.Thread.run(Unknown Source)
ğŸš€ 25/08/10 05:35:14 WARN FileSystem: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
ğŸš€ at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(Unknown Source)
ğŸš€ at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invoke(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/java.security.AccessController.doPrivileged(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invokePriv(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.login(Unknown Source)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
ğŸš€ at org.apache.hadoop.fs.viewfs.ViewFileSystem.<init>(ViewFileSystem.java:269)
ğŸš€ at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
ğŸš€ at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(Unknown Source)
ğŸš€ at java.base/java.util.ServiceLoader$ProviderImpl.get(Unknown Source)
ğŸš€ at java.base/java.util.ServiceLoader$3.next(Unknown Source)
ğŸš€ at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3379)
ğŸš€ at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3424)
ğŸš€ at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>(FsUrlStreamHandlerFactory.java:77)
ğŸš€ at org.apache.spark.sql.internal.SharedState$.liftedTree2$1(SharedState.scala:199)
ğŸš€ at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$setFsUrlStreamHandlerFactory(SharedState.scala:198)
ğŸš€ at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:54)
ğŸš€ at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
ğŸš€ at scala.Option.getOrElse(Option.scala:189)
ğŸš€ at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
ğŸš€ at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
ğŸš€ at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
ğŸš€ at scala.Option.getOrElse(Option.scala:189)
ğŸš€ at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
ğŸš€ at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
ğŸš€ at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
ğŸš€ at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
ğŸš€ at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
ğŸš€ at org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:572)
ğŸš€ at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:877)
ğŸš€ at org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:862)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Method.invoke(Unknown Source)
ğŸš€ at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
ğŸš€ at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
ğŸš€ at py4j.Gateway.invoke(Gateway.java:282)
ğŸš€ at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
ğŸš€ at py4j.commands.CallCommand.execute(CallCommand.java:79)
ğŸš€ at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
ğŸš€ at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
ğŸš€ at java.base/java.lang.Thread.run(Unknown Source)
ğŸš€ 25/08/10 05:35:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
ğŸš€ 25/08/10 05:35:14 INFO SharedState: Warehouse path is 'file:/tmp/spark-warehouse'.
ğŸš€ 25/08/10 05:35:14 INFO CodeGenerator: Code generated in 49.551167 ms
ğŸš€ ğŸ“Š DataFrame partitioning info:
ğŸš€ Subjects count: 1
ğŸš€ DataFrame partitions: 7
ğŸš€ 25/08/10 05:35:14 INFO SparkContext: Starting job: collect at /app/eeg_spark_etl/processing/process_subjects.py:76
ğŸš€ 25/08/10 05:35:14 INFO DAGScheduler: Got job 0 (collect at /app/eeg_spark_etl/processing/process_subjects.py:76) with 7 output partitions
ğŸš€ 25/08/10 05:35:14 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /app/eeg_spark_etl/processing/process_subjects.py:76)
ğŸš€ 25/08/10 05:35:14 INFO DAGScheduler: Parents of final stage: List()
ğŸš€ 25/08/10 05:35:14 INFO DAGScheduler: Missing parents: List()
ğŸš€ 25/08/10 05:35:14 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[9] at collect at /app/eeg_spark_etl/processing/process_subjects.py:76), which has no missing parents
ğŸš€ 25/08/10 05:35:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 16.7 KiB, free 434.4 MiB)
ğŸš€ 25/08/10 05:35:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 434.4 MiB)
ğŸš€ 25/08/10 05:35:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f1beba1b1841:38967 (size: 8.2 KiB, free: 434.4 MiB)
ğŸš€ 25/08/10 05:35:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1611
ğŸš€ 25/08/10 05:35:14 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 0 (PythonRDD[9] at collect at /app/eeg_spark_etl/processing/process_subjects.py:76) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
ğŸš€ 25/08/10 05:35:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 7 tasks resource profile 0
ğŸš€ 25/08/10 05:35:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f1beba1b1841, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes)
ğŸš€ 25/08/10 05:35:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (f1beba1b1841, executor driver, partition 1, PROCESS_LOCAL, 8979 bytes)
ğŸš€ 25/08/10 05:35:14 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (f1beba1b1841, executor driver, partition 2, PROCESS_LOCAL, 8979 bytes)
ğŸš€ 25/08/10 05:35:14 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (f1beba1b1841, executor driver, partition 3, PROCESS_LOCAL, 8979 bytes)
ğŸš€ 25/08/10 05:35:14 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (f1beba1b1841, executor driver, partition 4, PROCESS_LOCAL, 8979 bytes)
ğŸš€ 25/08/10 05:35:14 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (f1beba1b1841, executor driver, partition 5, PROCESS_LOCAL, 8979 bytes)
ğŸš€ 25/08/10 05:35:14 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (f1beba1b1841, executor driver, partition 6, PROCESS_LOCAL, 9118 bytes)
ğŸš€ 25/08/10 05:35:14 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
ğŸš€ 25/08/10 05:35:14 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
ğŸš€ 25/08/10 05:35:14 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
ğŸš€ 25/08/10 05:35:14 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
ğŸš€ 25/08/10 05:35:14 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
ğŸš€ 25/08/10 05:35:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
ğŸš€ 25/08/10 05:35:14 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
ğŸš€ 25/08/10 05:35:15 INFO CodeGenerator: Code generated in 5.831666 ms
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 364, boot = 287, init = 77, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 365, boot = 293, init = 72, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 367, boot = 284, init = 83, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 367, boot = 283, init = 84, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 362, boot = 285, init = 77, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 77, boot = 13, init = 64, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 377, boot = 298, init = 79, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 87, boot = 5, init = 81, finish = 1
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 91, boot = 21, init = 70, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 388, boot = 290, init = 97, finish = 1
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 101, boot = 10, init = 90, finish = 1
ğŸš€ 25/08/10 05:35:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1688 bytes result sent to driver
ğŸš€ 25/08/10 05:35:15 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1731 bytes result sent to driver
ğŸš€ 25/08/10 05:35:15 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1731 bytes result sent to driver
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 108, boot = 27, init = 81, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1731 bytes result sent to driver
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 108, boot = 33, init = 75, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1688 bytes result sent to driver
ğŸš€ 25/08/10 05:35:15 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1688 bytes result sent to driver
ğŸš€ 25/08/10 05:35:15 INFO PythonRunner: Times: total = 118, boot = 25, init = 93, finish = 0
ğŸš€ 25/08/10 05:35:15 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1877 bytes result sent to driver
ğŸš€ 25/08/10 05:35:15 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 508 ms on f1beba1b1841 (executor driver) (1/7)
ğŸš€ 25/08/10 05:35:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 511 ms on f1beba1b1841 (executor driver) (2/7)
ğŸš€ 25/08/10 05:35:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 518 ms on f1beba1b1841 (executor driver) (3/7)
ğŸš€ 25/08/10 05:35:15 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 513 ms on f1beba1b1841 (executor driver) (4/7)
ğŸš€ 25/08/10 05:35:15 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 515 ms on f1beba1b1841 (executor driver) (5/7)
ğŸš€ 25/08/10 05:35:15 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39123
ğŸš€ 25/08/10 05:35:15 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 515 ms on f1beba1b1841 (executor driver) (6/7)
ğŸš€ 25/08/10 05:35:15 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 514 ms on f1beba1b1841 (executor driver) (7/7)
ğŸš€ 25/08/10 05:35:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
ğŸš€ 25/08/10 05:35:15 INFO DAGScheduler: ResultStage 0 (collect at /app/eeg_spark_etl/processing/process_subjects.py:76) finished in 0.569 s
ğŸš€ 25/08/10 05:35:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
ğŸš€ 25/08/10 05:35:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
ğŸš€ 25/08/10 05:35:15 INFO DAGScheduler: Job 0 finished: collect at /app/eeg_spark_etl/processing/process_subjects.py:76, took 0.581038 s
ğŸš€ Rows per partition: [0, 0, 0, 0, 0, 0, 1]
ğŸš€ Processing subjects...
ğŸš€ ğŸ” Checking for existing processed subjects data...
ğŸš€ Reuse flag: No
ğŸš€ Check path: data/macMini2.0.0/processed_subjects
ğŸš€ 25/08/10 05:35:15 INFO CodeGenerator: Code generated in 7.5415 ms
ğŸš€ âš ï¸  Reuse not enabled for processed_subjects (flag: No)
ğŸš€ ğŸ”„ STEP 1: PROCEEDING - Processing subjects (feature extraction)...
ğŸš€ ğŸ”„ STEP 1: PROCESSING SUBJECTS - Feature extraction using UDTF...
ğŸš€ ğŸ”„ Using UDTF approach: SQL with TABLE ProcessSubjectUDTF
ğŸš€ ğŸ” SQL Query:
ğŸš€ -- Process subjects using UDTF for EEG feature extraction
ğŸš€ -- Config is passed via closure, not as a DataFrame column
ğŸš€ SELECT * FROM ProcessSubjectUDTF(TABLE(subjects_metadata))
ğŸš€ 25/08/10 05:35:15 INFO CodeGenerator: Code generated in 8.512417 ms
ğŸš€ âš ï¸  STEP 2: SAVE NOT CONFIGURED - Skipping save of processed subjects
ğŸš€ Set 'save_processed_subjects': 'Yes' in config to save data
ğŸš€ ğŸ”„ STEP 3: APPLYING FEATURE TRANSFORMATIONS...
ğŸš€ ğŸ’¾ STEP 4: SAVE CONFIGURED - Saving transformed features...
ğŸš€ Traceback (most recent call last):
ğŸš€ File "/app/main.py", line 201, in <module>
ğŸš€ main()
ğŸš€ File "/app/main.py", line 189, in main
ğŸš€ result: Dict[str, str] = process_subjects(spark, config_manager)
ğŸš€ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ğŸš€ File "/app/eeg_spark_etl/processing/process_subjects.py", line 242, in process_subjects
ğŸš€ transformed_df.write.parquet(str(save_transformed_path), mode="overwrite")
ğŸš€ File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1721, in parquet
ğŸš€ File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
ğŸš€ File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
ğŸš€ File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
ğŸš€ py4j.protocol.Py4JJavaError: An error occurred while calling o137.parquet.
ğŸš€ : java.lang.RuntimeException: org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
ğŸš€ at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(Unknown Source)
ğŸš€ at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invoke(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/java.security.AccessController.doPrivileged(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invokePriv(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.login(Unknown Source)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
ğŸš€ at org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72)
ğŸš€ at org.apache.hadoop.mapreduce.Job.<init>(Job.java:152)
ğŸš€ at org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195)
ğŸš€ at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:101)
ğŸš€ at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
ğŸš€ at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Method.invoke(Unknown Source)
ğŸš€ at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
ğŸš€ at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
ğŸš€ at py4j.Gateway.invoke(Gateway.java:282)
ğŸš€ at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
ğŸš€ at py4j.commands.CallCommand.execute(CallCommand.java:79)
ğŸš€ at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
ğŸš€ at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
ğŸš€ at java.base/java.lang.Thread.run(Unknown Source)
ğŸš€ at org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:74)
ğŸš€ at org.apache.hadoop.mapreduce.Job.<init>(Job.java:152)
ğŸš€ at org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195)
ğŸš€ at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:101)
ğŸš€ at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
ğŸš€ at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Method.invoke(Unknown Source)
ğŸš€ at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
ğŸš€ at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
ğŸš€ at py4j.Gateway.invoke(Gateway.java:282)
ğŸš€ at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
ğŸš€ at py4j.commands.CallCommand.execute(CallCommand.java:79)
ğŸš€ at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
ğŸš€ at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
ğŸš€ at java.base/java.lang.Thread.run(Unknown Source)
ğŸš€ Caused by: org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
ğŸš€ at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(Unknown Source)
ğŸš€ at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invoke(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/java.security.AccessController.doPrivileged(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invokePriv(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.login(Unknown Source)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
ğŸš€ at org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72)
ğŸš€ at org.apache.hadoop.mapreduce.Job.<init>(Job.java:152)
ğŸš€ at org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195)
ğŸš€ at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:101)
ğŸš€ at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
ğŸš€ at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Method.invoke(Unknown Source)
ğŸš€ at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
ğŸš€ at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
ğŸš€ at py4j.Gateway.invoke(Gateway.java:282)
ğŸš€ at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
ğŸš€ at py4j.commands.CallCommand.execute(CallCommand.java:79)
ğŸš€ at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
ğŸš€ at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
ğŸš€ at java.base/java.lang.Thread.run(Unknown Source)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1986)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
ğŸš€ at org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72)
ğŸš€ ... 45 more
ğŸš€ Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
ğŸš€ at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(Unknown Source)
ğŸš€ at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invoke(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/java.security.AccessController.doPrivileged(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invokePriv(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.login(Unknown Source)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
ğŸš€ at org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72)
ğŸš€ at org.apache.hadoop.mapreduce.Job.<init>(Job.java:152)
ğŸš€ at org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195)
ğŸš€ at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:101)
ğŸš€ at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
ğŸš€ at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
ğŸš€ at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
ğŸš€ at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
ğŸš€ at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
ğŸš€ at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
ğŸš€ at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
ğŸš€ at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
ğŸš€ at java.base/java.lang.reflect.Method.invoke(Unknown Source)
ğŸš€ at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
ğŸš€ at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
ğŸš€ at py4j.Gateway.invoke(Gateway.java:282)
ğŸš€ at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
ğŸš€ at py4j.commands.CallCommand.execute(CallCommand.java:79)
ğŸš€ at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
ğŸš€ at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
ğŸš€ at java.base/java.lang.Thread.run(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invoke(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext$4.run(Unknown Source)
ğŸš€ at java.base/java.security.AccessController.doPrivileged(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.invokePriv(Unknown Source)
ğŸš€ at java.base/javax.security.auth.login.LoginContext.login(Unknown Source)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
ğŸš€ at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
ğŸš€ ... 49 more
ğŸš€ 25/08/10 05:35:15 INFO SparkContext: Invoking stop() from shutdown hook
ğŸš€ 25/08/10 05:35:15 INFO SparkContext: SparkContext is stopping with exitCode 0.
ğŸš€ 25/08/10 05:35:15 INFO SparkUI: Stopped Spark web UI at http://f1beba1b1841:4040
ğŸš€ 25/08/10 05:35:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
ğŸš€ 25/08/10 05:35:15 INFO MemoryStore: MemoryStore cleared
ğŸš€ 25/08/10 05:35:15 INFO BlockManager: BlockManager stopped
ğŸš€ 25/08/10 05:35:15 INFO BlockManagerMaster: BlockManagerMaster stopped
ğŸš€ 25/08/10 05:35:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
ğŸš€ 25/08/10 05:35:15 INFO SparkContext: Successfully stopped SparkContext
ğŸš€ 25/08/10 05:35:15 INFO ShutdownHookManager: Shutdown hook called
ğŸš€ 25/08/10 05:35:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-aefe61b4-94cb-4428-9c23-6e6090332c1e
ğŸš€ 25/08/10 05:35:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-f930c24d-18ab-41f4-8003-57675e26d26b
ğŸš€ 25/08/10 05:35:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-aefe61b4-94cb-4428-9c23-6e6090332c1e/pyspark-8c73bf0a-f2e7-43cc-8125-4eb5f3e9fafd

âŒ Singularity command failed with exit code 1

ğŸ” Error details: Command '['singularity', 'run', '--bind', '/eeg-full-pipeline/config/config_macmini1.0.0_09-08-2025_1751.yaml:/app/config.yaml', '--bind', './logs/spark-events:/opt/bitnami/spark/logs/', '--bind', './data:/app/data', '--bind', '/Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set:/Volumes/CrucialX6/Home/bigData/ds004504-download/sub-001/eeg/sub-001_task-eyesclosed_eeg.set', './containers/eeg-pyspark-pipeline.sif', 'spark-submit', '--conf', 'spark.jars.ivy=/tmp/.ivy2', '/app/main.py', '--config', '/app/config.yaml']' returned non-zero exit status 1.

